{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshit0722/MediTalk/blob/main/MediTalk_execution_using_st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q bitsandbytes datasets accelerate loralib\n",
        "# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "id": "O2Lszgjw4vLP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IsNKc4-l3bQl"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# peft_model_id = \"Harshit0722/dolly-fine-tuned-on-med-data\"\n",
        "# config = PeftConfig.from_pretrained(peft_model_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# # Load the Lora model\n",
        "# model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "fGL8h0vmbLOr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rh5-oOzjm76",
        "outputId": "c26dffa4-5d5a-4d5a-ad00-e880ee6f6f43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st"
      ],
      "metadata": {
        "id": "L6uxmAx-lJ93"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "def main():\n",
        "    st.sidebar.title(\"Navigation\")\n",
        "    app_mode = st.sidebar.selectbox(\"Choose a page\", [\"Home\", \"About\", \"Contact\", \"Upload Files\"])\n",
        "\n",
        "    if app_mode == \"Home\":\n",
        "        home_page()\n",
        "    elif app_mode == \"About\":\n",
        "        about_page()\n",
        "    elif app_mode == \"Contact\":\n",
        "        contact_page()\n",
        "    elif app_mode == \"Upload Files\":\n",
        "        file_uploader_page()\n",
        "\n",
        "def home_page():\n",
        "    st.title(\"Home\")\n",
        "    st.write(\"Welcome to the home page!\")\n",
        "\n",
        "    # Add a text box for asking a question\n",
        "    question = st.text_input(\"Ask a question\")\n",
        "\n",
        " # Process the question (you can add your own logic here)\n",
        "    if question:\n",
        "        answer = process_question(question)\n",
        "        st.write(\"\", answer)\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"Harshit0722/dolly-fine-tuned-on-med-data\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "def process_question(question):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        batch = tokenizer(\"“Question asked is \" + question + \" ; Answer is ” ->: \", return_tensors='pt')\n",
        "        output_tokens = model.generate(**batch, max_new_tokens=75)\n",
        "\n",
        "        answer = (tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n",
        "        return answer\n",
        "\n",
        "def about_page():\n",
        "    st.title(\"About\")\n",
        "    st.write(\"This is a simple Streamlit application that demonstrates file uploading.\")\n",
        "\n",
        "def contact_page():\n",
        "    st.title(\"Contact\")\n",
        "    st.write(\"Contact us at example@example.com\")\n",
        "\n",
        "def file_uploader_page():\n",
        "    st.title(\"File Uploader\")\n",
        "\n",
        "    # File uploader\n",
        "    uploaded_file = st.file_uploader(\"Choose a file\", type=[\"txt\", \"csv\", \"pdf\", \"dcm\", \"nii\", \"nii.gz\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Display file details\n",
        "        file_details = {\n",
        "            \"Filename\": uploaded_file.name,\n",
        "            \"File size\": uploaded_file.size,\n",
        "            \"File type\": uploaded_file.type,\n",
        "        }\n",
        "        st.write(\"Uploaded file details:\")\n",
        "        st.write(file_details)\n",
        "\n",
        "        # Process the file (you can add your own logic here)\n",
        "        st.write(\"Processing the file...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "futfzK3jbLHL",
        "outputId": "66f22ed5-9a37-4856-b826-aa0fb150af94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch = tokenizer(\"“Question asked is \" + \"what is a cell?\" + \" ; Answer is ” ->: \", return_tensors='pt')\n",
        "\n",
        "# with torch.cuda.amp.autocast():\n",
        "#   output_tokens = model.generate(**batch, max_new_tokens=75)\n",
        "\n",
        "# print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "AJVGbGO8Brl6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer = ('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "Hs1yuTsKVWJs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(answer)"
      ],
      "metadata": {
        "id": "hOgRLdVXZuhd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "EEVEncTFb7gC",
        "outputId": "dc25a7fa-83f1-4cce-d81e-83a702d1d4cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.432s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "0EEcXWDzb8Gj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "WnSpp_x1b_eJ",
        "outputId": "c987fbb7-3a69-4325-af93-c23b983ad1c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your-ip-appears-here",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.261s\n",
            "your url is: follow this URL\n"
          ]
        }
      ]
    }
  ]
}
